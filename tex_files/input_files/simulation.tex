As said in section \ref{subsubsec:Discretization} the simulations are done with the Scalar-weak order 2.0 It√¥-Taylor method \cite[algorithm 8.5]{Srkk2019}. The concrete schemes that we derived for the respective types of noises are shown in equations (\ref{eq:OUSim} - \ref{eq:jacobiDiffusion}). A method for each scheme is implemented in \code{R}. All methods can be called with specific parameter choices, values for $\tau_c$ and $t_0$ as well as a temporal resolution, $\Delta t$ and starting point, $x_0$. \\Depending on the $\tau_c$, $t_0$, and $\Delta t$ the method dynamically picks the appropriate number of samples to draw for $\Delta W_k\sim\mathcal{N}\left(0,\Delta t\right)$. The methods allow for positive $\lambda_0$ and negative $A$, but recall that this only correspond to choosing between the positive and negative versions of (\ref{eq:standardStochasticForm}). If no starting point is specified the methods picks the fixed point in the stationary part of the process as the starting point; note that this of course is done with the appropriate branch of (\ref{eq:fixedPoint}) depending on the version of (\ref{eq:standardStochasticForm}), i.e. the sign of $A$. Additionally, an argument is implemented such that we can opt for stopping the process at earlier time (or later) points in time than $\tau_c$. This is useful, when we want to evaluate the models' ability to predict the tipping point based on how far from it we have samples. For details about the timing-benchmarks refer to section \ref{section:benchmark}. Specifications on hardware and software used can be found in table \ref{tab:specs}. 
\subsection{Overview of the estimation methods}
In this section, we test our methods on simulated data. The test aim to illustarte the robustness, computation efficiency and performance in terms of ARE (\ref{eq:ARE}). To perform the study for all the models and their respective methods at once, we sample with the parameters that worked for all models in figure \ref{figure:samplesFromAllDifferentModels}. That is $A = 1.5, m = 0.4, \lambda_0 = -0.2, \sigma = 0.1$, and we assumed throughout that $\nu = 1$, i.e. we do not estimate this parameter for now. According to the first-order taylor expansion, which we use in the stationary part (\ref{eq:TaylorStationary}), the true values are $\theta_{\mathrm{stationary}} = (1.095, 0.7651, 0.1)^\top$.  For the dynamic part, we simulate using $t_0 = 30$ and $\tau_c = 30$. We simulate with temporal resolutions $\Delta t = \{1/5, 1/10, 1/25, 1/100, 1/250\}$, corresponding to $N = \{150, 300, 750, 3000, 7500\}$ number of samples in each part of the process. For each value of $N$ we run the simulations and estimation $M = 50$ times and record the median ARE for each parameter. In addition, we calculate the median run-time for each method. In both cases we use the median due to it being more robust against outliers than the mean. Furthermore, due to the possibility of extreme samples for small $N$; we implement a \code{trycatch} suc that the program fails gracefully, if estimation fail for a model on a specific sample. For each run in the simulation all the models are simulated with the same realizations of the brownian motion; if one model fails the all models are re-run using a new seed. The optimizers are initialized at random points where each coordinate can deviate up to $\pm 10\%$ at random from the true values; this is done by sampling three uniform variables according to $U\sim\mathrm{Unif}(0.9, 1.1)$ and multiplying them with the true values.
As convergence criterion for the likelihood-based methods we use a relative tolerence of $\code{sqrt(.Machine\$double.eps)/10}\approx 1.49\cdot 10^{-9}$, while the estimation equations where an equation is numerically solved use $10^{-8}$; the standard of \code{nleqslv::nleqslv}. The likelihood based methods are the MLE and the Strang-based estimators; when there is more than one such method for a specific model, we denote it \textit{Likelihood (Alt.)}. Naturally, estimators based on the score equation or martingale estimation function based equation are estimation equation based estimators together.
\subsubsection{The stationary parts}
In the stationary parts, we run the simulations as described above and depict the median ARE as a function of sample size on a double logarithmic scale
\begin{figure}[h]
    \begin{center}
    \includegraphics[scale = .1]{figures/parameter_precision_stationary.jpeg}
    \caption{Overview of our estimation methods for the stationary parameters applied to the models}
    \label{figure:overviewOfEstimatorsStationary}
    \end{center}
\end{figure}\\
Note that the specific scale this experiment is done at likely affects the results. The impression of the estimators here likely does not generalized. To see why this might be the case, consider for instance how noisy the additive model is in figure \ref{figure:samplesFromAllDifferentModels} compared to figure \ref{figure:samplesFromFiveDifferentModels}, and then recall that we are in the situation in figure \ref{figure:samplesFromAllDifferentModels}. On the other scale in figure \ref{figure:samplesFromFiveDifferentModels}, the parameters of the additive model might be relatively easier to estimate than the other models.\\
Moving on, we cleary see that the $\alpha_0$-parameter inherently is more difficult to estimate. Additionally, figure \ref{figure:overviewOfEstimatorsStationary} reveals that it is only really the $\sigma$ parameter for which more samples improve the absolute relative error; the other parameters mostly stay at their respective levels in terms of median ARE. The $\mu_0$-parameter is generally not very troublesome to estimate - for any model it consistently has an ARE of around $10^{-2}$, which is around 1.5 orders of magnitude lower than the $\alpha_0$-parameter. Still, all the parameters are relatively well estimated in median, indicating that they are quite robust even for starting values that deviates somewhat. Again, this might stem from the fact that the models could behave especially nicely on the unit interval.\\
For the models with more than one method there is no significant difference in the median ARE for $\mu_0$ and $\sigma$. With regards to $\alpha_0$, the alternative Strang method (\ref{meanrevertingGBMSplit1}) faired quite a bit better with an ARE of around an order of magnitude lower than the AOMEF and Strang method with the heuristic from \cite{SplittingSchemes}. To see how the models compare computationally, we consider the median computation time for the different methods as a function of $N$
\begin{figure}[h]
    \begin{center}
    \includegraphics[scale = .075]{figures/estimation_duration_stationary.jpeg}
    \caption{Overview of our the median computation time for each method as a function of the number of samples}
    \label{figure:overviewOfDurationStationary}
    \end{center}
\end{figure}\\
This is the time from the invokation of the optimizer till the convergence criterion is reached. Generally, the estimation equations based estimators are much quicker than the likelihood based, but the explicit estimators based on (\ref{eq:squarerootMartingaleEquation}) for the square-root diffusion are naturally in a league of it owns being between 100 and 1000 times quicker than the likelihood based methods. The latter are generally in the same order of magnitude in terms of performance. Though, the $F$-, $t$- and Jacobi-diffusion based models are slower than the linear and square-root based models. This might be caused by the more complicated ordinary differential equations in these models, which the Runge-kutta needs to solve repeatedly; compare the expression in (\ref{eq:squarerootStationarySplit2}) and (\ref{eq:GBM_StrangNonLinearODE_split}) with (\ref{eq:StrangTDiffusion}), (\ref{eq:FscaledSplitting}) and (\ref{eq:jacobiODE}). If we look at the likelihood based methods that solves the ordinary differential equations numerically and contrast their computation time with the ones that are either use MLE or has a closed form solution to the ODE (\ref{meanrevertingGBMSplit1}), it is clear that there is a computational cost to solving the ODE numerically.
\subsubsection{The dynamic parts}
Turning to investigating the estimation in the dynamic part of each model, recall that we only use variations of the Strang-method for this. If more than one is used, it is denoted \textit{Strang (Alt.)} here. The alternative Strang estimators are the ones not based on the heuristic in \cite{SplittingSchemes}, i.e. the two estimators based on (\ref{eq:squareRootSplit2}) and (\ref{eq:GBMSplit2}) used in the square-root- and Linear models, respectively. We simulate from the models and fit in the manner described above. The ARE of the parameters in the dynamic part is depicted as a funtion of $N$ using a double logarithmic scale 
\begin{figure}[h!]
    \begin{center}
    \includegraphics[scale = .1]{figures/parameter_precision_dynamic.jpeg}
    \caption{ARE of the parameters of the models in the dynamic part}
    \label{figure:parameter_precision_dynamic}        
\end{center}
\end{figure}\\
It is generally much harder to estimate in the dynamic part. In terms of the $A$-parameter all models and type of methods faired relatively equally in terms of ARE. That is with the exception of the $t$-diffusion model, whose ARE are around half an order of magnitude smaller than that of the mother models. For some reason, the normal Strang estimator in the Square-root model as well as the $F$-diffusion based model show a relatively high median ARE for the largest number of samples we investigated.  Considering the other sample sizes this is more surprising for the Square-root noise model, as it is generally easier to estimate in. The $F$-diffusion seems, on this scale at least, to be the hardest of the models to do estimation on; whereas, as we mentioned, the $t$-diffusion based model seems the easiest. In terms of overall run time, we achieved similar results in terms of which kinds of models had faster convergence as we saw in figure \ref{figure:overviewOfDurationStationary}. For this reason, the results are only shown in appendix \ref{section:benchmark} in figure \ref{figure:estimation_duration_dynamic}. One surprise from this benchmark though, is the fact that $t$-diffusion based model, which solves its ODE numerically, has the fastest convergence of all the models. This probably stem from the model requiring less iterations overall before convergence is reached, as solving the ODE with Runge-kutta is much more computationally expensive than just computing the solution.

\subsection{Fitting the $\nu$-parameter}
We now consider how introducing the $\nu$-parameter into the model affects the performance of the optimizers. In this experiment we focus on the additive- and t-diffusion based models. Regardless of what is needed to achieve it, we ensure that the models that realizations we estimate on are the same for the models. We simulate from both models using $t_0 = 54$ and $\tau_c = 132$ and temporal resolutions $\Delta t = \{1/3, 1/5, 1/25, 1/50, 1/100\}$, which correspond to $N_\mathrm{dynamic} = \{396, 660, 3300, 6600, 13200\}$ samples in the dynamic part. The dynamic part is the important one here, because this is where $\nu$ is estimated. The other parameters are $\theta = (0.87, -1.51, -3,  0.387)^\top$. For each value of $\Delta t$ we investigate how the models and optimizers perform on $\nu = \{0.75, 0.9, 1, 1.11, 1.33\}$. Again, we consider the median of the ARE for each combination of $\Delta t$ and $\nu$. The experiments are again implemented such that failure to convergence or run the optimization results in a graceful exit. Now, instead of merely retrying we count how many time each model fails $N$ and $\nu$ to see, how diffuclt it is to get the methods to succeed in the estimation. The relative tolerance convergence criterion in \code{optim} was set to \code{sqrt(.Machine\$double.eps) / 1000}, for reasons that will become clear soon. We start by considering the ARE 
\begin{figure}[h!]
    \begin{center}
        \includegraphics[scale = .1]{figures/combined_nus_plot.jpeg}
        \caption{The absolute relative error of the different parameters as a function of $N$ for diffferent true values of $\nu$.}
        \label{figure:ARE_nu_plots}
    \end{center}
\end{figure}

\begin{figure}[h!]
    \begin{center}
        \includegraphics[scale = .1]{figures/error_count_plot.jpeg}
        \caption{The proportion of times where the respective models or both failed doing optimization as a function of $N$ and for different true values of $\nu$.}
        \label{figure:error_count_nu_experiment}
    \end{center}
\end{figure}

\subsection{Early warning analysis}

\subsection{The numerical Strang}

\subsection{Model misspecification}