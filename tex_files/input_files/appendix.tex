\section{Source code}
The source code for this project, licensed under the MIT License, is hosted on \href{https://github.com/Gantzhorn/Thesis}{Github}.
\section{Simulation Methods}\label{appendix:simMethods}
This section focuses on constructing the simulation schemes that support the analytical work presented in this thesis. We considered the Euler-maruyama - as well the Milstein scheme. However, as the processes are quite non-linear our final choice was the Scalar weak order 2.0 Itô–Taylor method \cite[algorithm 8.5]{Srkk2019}. In the general setup we simulate $X_{t_0},X_{t_1},\dots, X_{t_N}$ according to the schemes by sampling $N$ random variables $\Delta W_{t_k}\sim\mathcal{N}\left(0, \Delta t_k\right)$. Note that we let $X_{t_0} = \mu_0$, i.e. mean-reverting parameter in the stationary parts of the processes.
\noindent \textbf{Additive noise}\\
Invoking \cite[algorithm 8.5]{Srkk2019} we get the scheme
\begin{align}
    X_{t_{k + 1}} &= X_{t_k} - \left(A(X_{t_k} - m)^2 + \lambda_{t_k}\right) \Delta t_k + \sigma \Delta W_{k} \nonumber \\&-  A \left(X_{t_k} - m\right)\sigma \Delta W_k \Delta t_k\nonumber \\
    & + \left(A\left(X_{t_k} - m\right)\left(A\left(X_{t_k} - m\right)^2 + \lambda_{t_k}\right) - \frac{A \sigma^2}{2}\right)\left(\Delta t_k\right)^2 \label{eq:OUSim}
\end{align}
\\
\textbf{Square-root noise}\\
For the square-root based process the algorithm gives 
\begin{align}
    X_{t_{k + 1}} &= X_{t_k} - \left(A(X_{t_k} - m)^2 + \lambda_{t_k}\right) \Delta t_k + \sigma \sqrt{X_t} \Delta W_{k}\nonumber\\ &+ \frac{1}{4}\sigma^2 \left(\Delta W_k^2 - \Delta t_k\right)     - A\left(X_{t_k} - m\right)\sigma \sqrt{X_{t_k}} W_k \Delta t_k
    \nonumber\\
     &+ \left(A\left(X_{t_k} - m\right)\cdot \left(A\left(X_{t_k} - m\right)^2 + \lambda_{t_k}\right) - \frac{A\sigma^2 X_{t_k}}{2}\right)(\Delta t_k)^2 \nonumber\\
    &- \frac{1}{4\sqrt{X_{t_k}}}\left(\sigma\left(A\left(X_{t_k} - m\right)^2 + \lambda_{t_k}\right) + \frac{1}{4}\sigma^3\right) \left(\Delta W_k \Delta t_k\right)
\end{align}
\\
\textbf{Linear noise}\\
With linear noise we get the simulation scheme
\begin{align}
    X_{t_{k + 1}} &= X_{t_k} - \left(A(X_{t_k} - m)^2 + \lambda_{t_k}\right) \Delta t_k + \sigma X_t \Delta W_{k} \nonumber \\ &
    + \frac{1}{2}\sigma^2 X_{t_k}\left(\Delta W_{k}^2 - \Delta t_k\right) -A(X_{t_k} - m)\sigma X_{t_k} \Delta W_k\Delta t_k \nonumber\\
    & + \left(A\left(X_{t_k} - m\right)\left(A\left(X_{t_k} - m\right)^2 + \lambda_{t_k}\right) - \frac{A\sigma^2X_{t_k}^2}{2}\right)(\Delta t_k)^2 \nonumber \\
    &- \frac{\sigma}{2}\left(A\left(X_{t_k} - m\right)^2 + \lambda_{t_k}\right)\left(\Delta W_{k}\Delta t_k\right).
\end{align}
\\
\textbf{Skew t-distribution}\\
With the skew t distribution as the Stationary distribution of the non-dynamic part of the process we get the following scheme
\begin{align}
    X_{t_{k + 1}} &= X_{t_k} - \left(A(X_{t_k} - m)^2 + \lambda_{t_k}\right) \Delta t_k + \sigma \sqrt{\left(X_{t_k}^2 + 1\right)} \Delta W_{k} \nonumber\\
    &+ \frac{\sigma^2}{2}X_t \left(\Delta W_{k}^2 - \Delta t_k\right) - A\left(X_{t_k} - m \right)\sigma\sqrt{\left(X_{t_k}^2 + 1\right)}\Delta W_{k}\Delta t_k \nonumber\\
    &+ \left(\left(A\left(X_{t_k} - m \right)^2+\lambda_{t_k}\right)\left(A\left(X_{t_k} - m \right)\right) - \frac{A\sigma^2}{2}\left(X_t^2 + 1\right)\right)\left(\Delta t_k\right)^2 \nonumber\\
    &-\left(\sigma X_{t_k}\left(A\left(X_{t_k} - m \right)^2 + \lambda_{t_k}\right) - \frac{\sigma^3}{2}\right)\frac{\left(\Delta W_{k}\Delta t_k\right)}{2\sqrt{X_{t_k}^2 + 1}}
\end{align}
\\
\textbf{F-distribution}\\
The model with the F-distribution as the stationary distribution in the non-dynamic part is sampled in the following manner
\begin{align}
    X_{t_{k + 1}} &= X_{t_k} - \left(A(X_{t_k} - m)^2 + \lambda_{t_k}\right) \Delta t_k + \sigma\sqrt{X_{t_k}\left(X_{t_k} + 1\right)}\Delta W_k \nonumber \\
    &+ \frac{\sigma^2}{4}\left(2X_{t_k} + 1\right)\left(\Delta W_k^2 - \Delta t_k\right) - A \left(X_{t_k} - m\right)\sigma \sqrt{X_{t_k}\left(X_{t_k} + 1\right)}\Delta W_k \Delta t_k \nonumber \\
    &+ \left(\left(A\left(X_{t_k} - m\right)^2 + \lambda_{t_k}\right)\left(A\left(X_{t_k} - m\right)\right) - \frac{\sigma^2A}{2}\left(X_{t_k}\left(X_{t_k} + 1\right)\right)  \right)\left(\Delta t_k\right)^2 \nonumber \\
    &- \left(\sigma\left(A\left(X_{t_k} - m\right)^2 + \lambda_{t_k}\right)\left(2 X_{t_k} + 1\right) + \frac{\sigma^3}{4}\right)\frac{\Delta W_k \Delta t_k}{4\sqrt{X_{t_k}\left(X_{t_k} + 1\right)}}
\end{align}
\\
\textbf{Jacobi-diffusion}\\
Basing the modelling on the jacobi diffusion yields a scheme
\begin{align}
    X_{t_{k + 1}} &= X_{t_k} - \left(A(X_{t_k} - m)^2 + \lambda_{t_k}\right) \Delta t_k + \sigma \sqrt{X_{t_k}\left(1-X_{t_k}\right)} \Delta W_{k} \nonumber\\
    &+ \frac{\sigma^2}{4}\left(1 - 2X_{t_k}\right)\left(\Delta W_k^2 - \Delta t_k\right) - A\left(X_{t_k} - m\right)\sigma \sqrt{X_{t_k}\left(1 - X_{t_k}\right)}\Delta W_k\Delta t_k \nonumber \\
    &+ \left(\left(A\left(X_{t_k} - m\right)^2 + \lambda_{t_k}\right)\left(A\left(X_{t_k} - m\right)\right) - \frac{\sigma^2A}{2}\left(X_{t_k}\left(1 - X_{t_k}\right)\right)\right)\left(\Delta t_k\right)^2 \nonumber \\
    &- \left(\left(A\left(X_{t_k} - m\right)^2 + \lambda_{t_k}\right)\sigma\left(1 - 2X_{t_k}\right) + \frac{\sigma^3}{2}\right)\frac{\Delta W_k \Delta t_k}{4\sqrt{X_{t_k}\left(1 - X_{t_k}\right)}} \label{eq:jacobiDiffusion}
\end{align}
\section{Derivation of Estimators}
Here, we formulate the estimators that have been employed throughout this study deriving them by means of classic- and novel methods.
\subsection{Ornstein-Uhlenbeck}\label{subsec:OUprocess}
\subsubsection{Stationary part}\label{subsubsec:OUprocessStationary}
In the stationary part of the process we may assume that the process behaves like an Ornstein-Uhlenbeck process. 
\begin{align}
    \mathrm{d}X_t = -\alpha_0\left(X_t-\mu\right) \mathrm{d}t + \sigma \mathrm{d}W_t.
\end{align}
Let $\rho = \exp\left(-\alpha_0\Delta\right), \gamma^2 = \sigma^2/2\alpha_0$. This process has transition density 
\begin{align}
    p\left(\Delta, \mathbf{x};\theta\right) = \frac{1}{\sqrt{2\pi\gamma^2\left(1-\rho^2\right)}}\exp\left(-\frac{\left(x_i-x_{i-1}\rho - \mu\left(1-\rho\right)\right)^2}{2\gamma^2\left(1-\rho^2\right)}\right), \label{eq:OULikelihood}
\end{align}
which is a gaussian density with conditional mean and -variance
\begin{align}
    \mathbb{E}\left[X_i\middle|X_{i-1} = x_{i-1}\right] &= x_{i - 1}\rho - \mu\left(1-\rho\right),\\
    \mathrm{Var}\left(X_i\middle|X_{i-1} = x_{i-1}\right) &= \gamma^2\left(1-\rho^2\right).
\end{align}
Unfortunately, we cannot get closed form solution to all of the estimators from this. However, we can still maximize (\ref{eq:OULikelihood}) with respect to $\theta$ to get the MLEs. Alternatively, we can derive the score equations by taking the derivative of the log-likelihood, and numerically solve the non-linear equations we get from doing so. The score equations are present in \cite{DitlevsenSupplementary}. To solve the equations, we use the $\code{nleqslv::nleqslv}$-method \cite{nleqslv}. For starting values in both methods we use the approximation for the MLE of $\mu$ in equation \cite[(S4)]{DitlevsenSupplementary} along with (S4),(S5) for $\rho, \gamma^2$. 
\subsubsection{Dynamic part part}\label{subsubsec:OUprocessDynamic}
This part of the estimation procedure has to do with the estimation of the parameters linked to the dynamic part of the process
\begin{align}
    \mathrm{d}X_t &= -\left(A\left(X_t - m\right) + \lambda_t\right) + \sigma \mathrm{d}W_t,\\
    \lambda_t &= \lambda_0\left(1 - \frac{t - t_0}{\tau_c}\right)^\nu,
\end{align}
\subsection{Square-root process}\label{subsec:squareroot}
\subsubsection{Stationary part}\label{subsubsec:squarerootStationary}
In the stationary part we assume the process can be modelled by the square-root process
\begin{align}
    \mathrm{d}X_t &= -\alpha_0\left(X_t - \mu_0\right) + \sigma \sqrt{X_t} \mathrm{d}W_t. \label{eq:squarerootAppendix}
\end{align}
\textbf{The Strang-based Estimator}\\
To obtain the strang estimator, we need additive noise. The lamperti transformation of (\ref{eq:squarerootAppendix}) is $Y_t := 2\sqrt{X_t}$. By Itô's formula
\begin{align}
    \mathrm{d}Y_t = - \frac{1}{Y_t}\left(\alpha_0 \frac{Y_t^2}{2} - 2 \alpha_0 \mu_0 + \frac{\sigma^2}{2}\right)\mathrm{d}t + \sigma \mathrm{d}W_t. \label{eq:lampertiSquarerootAppendix}
\end{align}
Using the heuristic from \cite[section 2.3 and 2.5]{SplittingSchemes} we let the Ornstein-Uhlenbeck part be the linearization around the fixed point of the drift.
\begin{align}
    \mathrm{d}Y_t^{[1]} &= -\alpha_0 \left(Y_t^{[1]} + \sqrt{\frac{4\alpha_0 \mu_0 - \sigma^2}{\alpha_0}}\right) + \sigma \mathrm{d}W_t , \label{eq:squarerootStationarySplit1} \\
    \mathrm{d}Y_t^{[2]} &= \frac{\alpha_0 Y_t^{[2]}}{2} + \frac{4\alpha_0 \mu_0 - \sigma^2}{2 Y_t} + \sqrt{\alpha_0\left(4\alpha_0\mu_0 - \sigma^2\right)}. \label{eq:squarerootStationarySplit2}
\end{align}
Due to the nature of the lamperti-transformation giving us non-negative values, we only consider the positive branch of the two fixed points of \ref{eq:lampertiSquarerootAppendix}. The deterministic ODE in (\ref{eq:squarerootStationarySplit2}) is the residual of (\ref{eq:lampertiSquarerootAppendix}) and (\ref{eq:squarerootStationarySplit1}).
Equation (\ref{eq:squarerootStationarySplit1}) represents an Ornstein-Uhlenbeck process, which with stepsize, $\Delta t$, has a gaussian flow with mean 
\begin{align}
    \mu_{\Delta t}(x) = \exp\left(-\alpha_0 \Delta t\right) \left(x + \sqrt{\frac{4\alpha_0\mu_0 - \sigma^2}{\alpha_0}}\right) - \sqrt{\frac{4\alpha_0\mu_0 - \sigma^2}{\alpha_0}}
\end{align}
and variance
\begin{align}
     \Omega_{\Delta t} = \frac{\sigma^2\left(1 - \exp\left(-2\alpha_0 \Delta t\right)\right)}{2\alpha_0}
\end{align}
\cite[(5), (6)]{SplittingSchemes}. On the other hand, we did not find an analytical solution to (\ref{eq:squarerootStationarySplit2}), but we will instead rely on the classical Fourth-Order Runge-Kutta Method \cite[p. 541]{numericalAnalysis} to find the flow. Nevertheless, we can denote these two flows $\varphi_{\Delta t}^{[1]}, \varphi_{\Delta t}^{[2]}$ respectively and the Strang splitting scheme is then the composition
\begin{align}
    X_{t_{k + 1}} = \left(\varphi_{\Delta t}^{[2]} \circ \varphi_{\Delta t}^{[1]} \circ \varphi_{\Delta t}^{[2]}\right)\left(X_{t_k}\right).
\end{align}
That gives us our psuedo-likelihood; a non-linear transformation of a gaussian random variable. The maximizer of this function is then our estimator. \\


\noindent \textbf{Approximate Quadratic Martingale Estimating Equation}\\
We start by deriving the expressions for the one-step ahead conditional mean and variances for the square-root process. The generator for our process is
\begin{align}
    \mathcal{L}(f) = \frac{1}{2}\sigma^2x\frac{\mathrm{d}}{\mathrm{d}x^2}f -\beta\left(x - \mu\right)\frac{\mathrm{d}}{\mathrm{d}x}f.
\end{align}
we say that $\varphi$, $\lambda$ are eigen functions and -values for the process if
\begin{align}
    \mathcal{L}\varphi = - \lambda \varphi,
\end{align}
and under mild regulatory conditions \cite[theorem 1.16]{StatisticalMethodsForSDE} gives a method to derive the moments as
\begin{align}
    \mathbb{E}\left[\varphi(X_{t_k + 1}) \middle | X_{t_k}\right] = \exp\left(-\lambda t\right)\varphi \label{eq:momentConditions}
\end{align}
For typographical reasons, the dependence on the parameters have been suppresed in both $\varphi$ and $\lambda$. Additionally, Forman and Sorensen \cite{FormanSorensen2008} showed that for simple diffusions, such as the square-root process, the eigenfunctions are all polynomials. Since we only will be constructing a quadratic martingale estimation equation, we only need the first two eigen functions and -values. By direct computation one easily finds that 
\begin{align}
    \phi_1(x) &= x-\mu, &&\; \lambda_1 = \beta, \label{squarerooteigen1}\\
    \phi_2(x) &= x^2 - \left(2\mu + \frac{\sigma^2}{\beta}\right)x + \mu^2 + \frac{\sigma^2\mu}{2\beta}, &&\; \lambda_2 = 2\beta. \label{squarerooteigen2}
\end{align}
Alternatively, we can also verify these results by inserting into the definition
\begin{align}
    \mathcal{L}\phi_1(x) = -\beta\left(x - \mu\right)\cdot 1 + \frac{1}{2}\sigma^2 x \cdot 0 = -\beta\left(x - \mu\right) = -\lambda_1\phi_1(x).
\end{align}
Additionally,
\begin{align}
    \mathcal{L}\phi_2(x) &= -\beta\left(x - \mu\right)\cdot \left(2x -2\mu - \frac{\sigma^2}{\beta}\right) + \sigma^2x\\
    =& -2\beta\left(x^2-x\mu - x\mu + \mu^2 -\frac{\sigma^2}{2\beta}x + \frac{\sigma^2}{2\beta}\mu - \frac{\sigma^2}{2\beta}x \right)\\
    &= -2\beta \left(x^2 -\left(2\mu+\frac{\sigma^2}{\beta}\right)x + \mu^2 + \frac{\sigma^2\mu}{2\beta}\right) = -\lambda_2\phi_2(x).\\
\end{align}
Now using (\ref{eq:momentConditions}) the conditional mean can be calculated as
\begin{align}
    E\left[X_\Delta - \mu \middle| X = x\right] &= \exp\left(-\beta\Delta\right)\left(x-\mu\right),\\
    E\left[X_\Delta \middle| X = x\right] &= \exp\left(-\beta\Delta\right)\left(x-\mu\right) + \mu. \label{squarerootCondMean}
\end{align}
Similarly, the conditional one-step ahead second moment is found
\begin{align}
    &\mathbb{E}\left[X_\Delta^2 - \left(2\mu + \frac{\sigma^2}{\beta}\right)X_\Delta + \mu^2 + \frac{\sigma^2\mu}{2\beta} \middle| X = x \right] \nonumber \\
    &= \exp\left(-2\beta \Delta\right)\left(x^2 - \left(2\mu + \frac{\sigma^2}{\beta}\right)x + \mu^2 + \frac{\sigma^2 \mu}{2\beta}\right),\\
    \mathbb{E}\left[X_\Delta^2 \middle| X = x\right] &= \exp\left(-2\beta \Delta\right)\left(x^2 - \left(2\mu + \frac{\sigma^2}{\beta}\right)x + \mu^2 + \frac{\sigma^2 \mu}{2\beta}\right)  \nonumber  \\
     &+ \left(2\mu + \frac{\sigma^2}{\beta}\right) \left(\exp\left(-\beta\Delta\right)\left(x-\mu\right) + \mu\right) - \left(\mu^2 + \frac{\sigma^2\mu}{2\beta}\right). \label{squarerootCondsecondMoment}
\end{align}
By definition of conditional variance and combining (\ref{squarerootCondsecondMoment}) and (\ref{squarerootCondMean}), one easily gets
\begin{align}
    \mathrm{Var}\left(X_\Delta \middle| X = x \right) = \frac{\sigma^2}{\beta}\left(\exp\left(-2\beta\Delta\right)\left(\frac{\mu}{2} - x   \right) - \exp\left(-\beta\Delta \right)\left(\mu - x\right) + \frac{\mu}{2}\right).
\end{align}
Invoking the result from \cite[Example 1.11]{StatisticalMethodsForSDE} and denoting the conditional mean and -variance, $F_i, \Phi_i$, respectively we get an approximate quadratic martingale estimation equation
\begin{align}
        G_N^{sq} = \begin{bmatrix}
            \sum_{i = 1}^N \frac{\beta}{\sigma^2 X_{t_{i-1}}}\left(X_{t_i} - F_{t_i}\right)\\
            \sum_{i = 1}^N \frac{-1}{\sigma^2}\left(X_{t_i} - F_{t_i}\right)\\
            \sum_{i = 1}^N \frac{1}{\sigma^3 X_{t_{i-1}}}\left(\left(X_{t_i} - F_{t_i}\right)^2 - \Phi_{t_i}\right) \label{eq:squarerootMartingaleEquation}
        \end{bmatrix}.
\end{align}
Setting $G_N^{sq} = 0$ we get the following results
\begin{align}
    \exp\left(-\hat{\beta}_N\Delta\right) &= \frac{N\sum_{i=1}^{N}X_{t_i} / X_{t_{i - 1}} - \left(\sum_{i = 1}^{N}X_{t_i}\right)\left(\sum_{i}^{N}1/X_{t_{i - 1}}\right)}{N^2 - \left(\sum_{i = 1}^{n}X_{t_i}\right)\left(1/X_{t_{i - 1}}\right)},\\
    \hat{\mu}_N &= \frac{\sum_{i = 1}^{N}X_{t_i} / X_{t_{i - 1}} - N \exp\left(-\hat{\beta}_N\Delta\right)}{\left(1-\exp\left(-\hat{\beta}_N\Delta\right)\right)\sum_{i = 1}^{N}1/X_{t_{i - 1}}}.
\end{align}
For the two parameters related to the drift of the process. Define
\begin{align}
    \Phi^*(\Delta, x; \theta) := \left(\frac{x}{\beta}\left(\exp\left(-\beta\Delta - \exp\left(-2\beta \Delta\right)\right)\right) + \frac{\mu}{2\beta}\left(1-\exp\left(-\beta\Delta\right)\right)^2\right).
\end{align}
Then the third equation in (\ref{eq:squarerootMartingaleEquation}) yields the explicit equation for third noise-parameter
\begin{align}
    \hat{\sigma}^2_N = \frac{\sum_{i = 1}^{N}\frac{1}{X_{t_{i - 1}}}\left(X_{t_i} - F_i\right)^2}{\sum_{i = 1}^{N}\frac{1}{X_{t_{i - 1}}}\Phi^*(\Delta, x; \theta)}
\end{align}
\subsubsection{Dynamic part}\label{subsubsec:squarerootDynamic}
In its dynamic part the process is governed by the stochastic differential equation
\begin{align}
    \mathrm{d}X_t &= -\left(A\left(X_t - m\right) + \lambda_t\right) + \sigma \sqrt{X_t} \mathrm{d}W_t, \label{eq:dynamicsquarerootSDE}\\
    \lambda_t &= \lambda_0\left(1 - \frac{t - t_0}{\tau_c}\right)^\nu,
\end{align}
and we assume that we already estimations from the stationary part i.e. that $\lambda_0, \sigma, m$ are estimated or can be computed directly from other parameters. That is from \ref{subsubsec:squarerootStationary} we have the estimates $\alpha_0, \mu_0, \sigma$ and we can compute
\begin{align}
    m &= \mu_0 - \frac{\alpha_0}{2A},\\
    \lambda_0 &= - \frac{\alpha_0^2}{4A}.
\end{align}
In other words, our objective is to estimate $A, \tau_c, \nu$, where $\tau_c$ obviously is our parameter of interest. The drift term is the same as in \cite{Ditlevsen2023}, thus in complete analogy to \cite[(S9, S10)]{DitlevsenSupplementary} we split the system into
\begin{align}
    \mathrm{d}X_t^{[1]} &= -\alpha(\lambda)\left(X_t^{[1]} - \mu(\lambda)\right)  \mathrm{d}t + \sigma \sqrt{X_t^{[1]}} \mathrm{d}W_t, \label{eq:squareRootSplit1} \\
    \mathrm{d}X_t^{[2]} &= - A \left(X_t^{[2]} - \mu(\lambda)\right)^2 \mathrm{d}t, \label{eq:squareRootSplit2}
\end{align}
with $\alpha(\lambda) = 2\sqrt{\abs{A\lambda_t}}$ and $\mu(\lambda) = m + \sqrt{\abs{\frac{\lambda_t}{A}}}$.
This particular choice of splitting is done by letting the square-root process part be the linearization around the fixed point of the process. This splitting is originally motivated in \cite[section 2.3 and 2.5]{SplittingSchemes}. It is evident that equation (\ref{eq:squareRootSplit2}) is identical to equation \cite[(S10)]{DitlevsenSupplementary}. However, the square-root dependency highlighted in (\ref{eq:squareRootSplit1}) yields a flow that is different to the flow of \cite[(S9)]{DitlevsenSupplementary}. To address this, we adopt the approach developed by Kessler \cite{Kessler1997} to (\ref{eq:squareRootSplit1}). This method approximates the transition density by a gaussian density qua the true mean and variance, which we from \ref{subsubsec:squarerootStationary} have readily available. Denoting the flow with steplength, $\Delta t$, of \ref{eq:squareRootSplit1} and \ref{eq:squareRootSplit2} $\varphi_{\Delta t}^{(1)}, \varphi_{\Delta t}^{(2)}$ respectively the Strang based flow is 
\begin{align}
    \left(X_{t_n + \Delta t} | X_{t_n} = x\right) = \left(\varphi_{\Delta t/2}^{(2)} \circ \varphi_{\Delta t}^{(1)} \circ \varphi_{\Delta t/2}^{(2)}\right)(x),
\end{align} 
which gives rise to a pseudo-likelihood similar to \cite[(14)]{SplittingSchemes}. The maximizer of the pseudo-likelihood is the Strang-based estimator.\\
On the other hand, we can also utilize the lamperti-transform on the dynamic part of the process to get a strang-based estimator. Let $Y_t:= 2\sqrt{X_t}$ and use Itô's formula on (\ref{eq:dynamicsquarerootSDE}), then
\begin{align}
    \mathrm{d}Y_t = - \frac{2}{Y_t}\left(A\left(\frac{Y_t^2}{4} - m\right)^2 + \lambda_t + \frac{\sigma^2}{4}\right)\mathrm{d}t + \sigma \mathrm{d}W_t.
\end{align}
Linearizing this around the fix point of the drift, we get the following splitting scheme
\begin{align}
    \mathrm{d}Y_t^{[1]} &= A_{\mathrm{linear\_part}}\left(Y_t^{[1]} - b_{\mathrm{intercept}}\right)\mathrm{d}t + \sigma \mathrm{d}W_t,\\
    \mathrm{d}Y_t^{[2]} &= - \left(\frac{2}{Y_t}\left(A\left(\frac{Y_t^2}{4} - m\right)^2 + \lambda_t + \frac{\sigma^2}{4}\right) + A_{\mathrm{linear\_part}}\left(Y_t^{[1]} - b_{\mathrm{intercept}}\right)\right)\mathrm{d}t,
\end{align}
where 
\begin{align}
    A_{\mathrm{linear\_part}} &= \frac{\lambda_t \left(1 - 4A^2\right) + \sigma^2 \left(1/4 - A^2\right)}{2m + \sqrt{-A\left(\sigma^2 + 4 \lambda_t\right)}} - A \sqrt{-A\left(\sigma^2 + 4 \lambda_t\right)},  \\
    b_{\mathrm{intercept}} &= \sqrt{4m + 2 \sqrt{-A\left(\sigma^2 + 4 .\lambda_t\right)}}
\end{align}
We solve these flows using the solution to the Ornstein-Uhlenbeck process and a fourth-order Runge-kutta respectively. The solutions in the usual composition then gives us the Strang-flow, which we maximize in order to get the Strang-based estimator for the transformed process.
\subsection{Mean-reverting Geometric Brownian Motion}\label{subsec:meanrevertingGBM}
\subsubsection{Stationary part}\label{subsubsec:meanrevertingGBMStationary}
In the stationary part we assume the process can be modelled by the mean-reverting Geometric Brownian Motion.
\begin{align}
    \mathrm{d}X_t &= -\alpha_0\left(X_t - \mu_0\right) + \sigma X_t \mathrm{d}W_t
\end{align}
For this process we present three estimation methods in total.\\
\noindent \textbf{Strang splitting estimators}\\
Here we present two estimators based on different splitting philosophies. First we consider a naïve splitting. That is, we split the mean-reverting GBM into
\begin{align}
    \mathrm{d}X_t^{[1]} = \sigma X_t^{[1]}\mathrm{d}W_t \label{meanrevertingGBMSplit1}\\
    \mathrm{d}X_t^{[2]} = -\beta\left(X_t^{[2]} - \mu\right)\mathrm{d}t \label{meanrevertingGBMSplit2}
\end{align}
According to \cite[example 4.7]{Srkk2019} (\ref{meanrevertingGBMSplit1}) has solution 
\begin{align}
    X_t = \exp\left(\log(x_0) -\frac{\sigma^2}{2}t + \sigma W_t\right),
\end{align}
which is log-normal with parameters $\log(x_0) -\frac{\sigma^2}{2}t$ and $\sigma$ respectively.
Using $X_0 = x_0$ we get that the solution to the ODE (\ref{meanrevertingGBMSplit2}) is
\begin{align}
    f_{\Delta t}(x_0) &= (x_0 - \mu)\exp(-\beta\Delta) + \mu\\
    f_{\Delta t}^{-1}(x_1) &=  (x_1 - \mu)\exp(\beta\Delta) + \mu\\
    D_x f_{\Delta t}^{-1}(x_1) &= \exp\left(\beta\Delta t\right),
\end{align}
thus giving the Strang based negative log-likelihood
\begin{align}
    l_S(\beta, \mu, \sigma) = \sum_{i = 1}^{N} g(f_{\Delta t}^{-1}(x_i); \beta,\mu, \sigma) - \frac{\beta\Delta t(N - 1)}{2},
\end{align}
where $g$ is minus the logarithm of the log-normal density and $N$ the number of samples.
Conversely, we can use the lamperti transformation of the mean-reverting GBM and the familiar Strang likelihood. The lamperti transform is $Y_t = \log(X_t)$ and by Itôs formula the resulting SDE is
\begin{align}
    \mathrm{d}Y_t = -\left(\beta\left(1 - \mu\exp(-Y_t)\right) + \frac{1}{2}\sigma^2\right)\mathrm{d}t + \sigma \mathrm{d}W_t.
\end{align}
Again, using the heuristic from \cite{SplittingSchemes} and linearizing around the fixed-point we get the splitting
\begin{align}
    \mathrm{d}Y_t^{[1]} &= - \frac{\sigma^2 + 2\beta}{2}\left(Y_t^{[1]} + \log\left(\frac{\sigma^2 + 2\beta}{2\beta \mu}\right)\right)\mathrm{d}t + \sigma \mathrm{d}W_t\\
    \mathrm{d}Y_t^{[2]} &= \left(-\beta + \beta\mu\exp(-Y_t) - \frac{\sigma^2}{2} + \frac{\sigma^2+2\beta}{2}Y_t^{[2]} + \frac{\sigma^2+2\beta}{2}\log\left(\frac{\sigma^2+2\beta}{2\beta\mu}\right)\right)\mathrm{d}t,
\end{align}
which we solve using the linear SDE and a fourth order Runge-kutta respectively.\\

\noindent \textbf{Approximate Quadratic Martingale Estimating Equation}\\
\noindent Starting once more with deriving the one-step ahead conditional mean and - variance. The generator is
\begin{align}
    \mathcal{L}\varphi = -\beta\left(x - \mu\right)\frac{\mathrm{d}}{\mathrm{d}x}\varphi + \frac{1}{2}\sigma^2 x^2 \frac{\mathrm{d}^2}{\mathrm{d}x^2}\varphi,
\end{align}
whence we get the eigen functions and -values.
\begin{align}
    \varphi_1(x) &= x - \mu, \qquad &&\lambda_1 = \beta, \label{GBMeigen1} \\
    \varphi_2(x) &= x^2 - \frac{2\beta\mu}{\beta - \sigma^2}x + \frac{2\mu^2\beta^2}{\left(\beta - \sigma^2\right)\left(2\beta - \sigma^2\right)}, \qquad &&\lambda_2 = 2\beta - \sigma^2 \label{GBMeigen2}.
\end{align}
Evidently, the verification done on (\ref{squarerooteigen1}) can be done completely analagously on (\ref{GBMeigen1}). Instead, we verify that (\ref{GBMeigen2}) contains the second eigen function and -value
\begin{align}
    \mathcal{L}\varphi_2 & = -\beta\left(x-\mu\right)\left(2x - \frac{2\beta\mu}{\beta-\sigma^2}\right) + \sigma^2x^2,\\
    &=-\left(2\beta - \sigma^2\right)x^2 + \frac{2\beta\mu\left(2\beta-\sigma^2\right)}{\beta - \sigma^2}x - \frac{2\beta^2\mu^2}{\beta - \sigma^2}, \\
    &= -\left(2\beta -\sigma^2\right)\left(x^2 - \frac{2\beta\mu}{\beta-\sigma^2} + \frac{2\beta^2\mu^2}{\left(\beta-\sigma^2\right)\left(2\beta-\sigma^2\right)}\right),\\
    &= - \lambda_2 \varphi_2.
\end{align}
Since it has the same first eigen function the mean-reverting GBM has the sam one-step ahead conditional mean as (\ref{squarerootCondMean}). In fact, it is not difficult to see that this result generalizes to any pearson diffusion. Conversely, we can calculate the one-step ahead conditional second moment by realizing
\begin{align}
    \mathbb{E}\left[\varphi_2(X_\Delta)\middle|X = x\right] &= \exp\left(-\left(2\beta - \sigma^2\right)\right)\varphi_2(x),
\end{align}
and thus
\begin{align}
    \mathbb{E}\left[X_\Delta^2 \middle| X = x\right] &= \exp\left(-\left(2\beta - \sigma^2\right)\right)\varphi_2(x) + \frac{2\beta\mu}{\beta-\sigma^2} \mathbb{E}\left[X_\Delta\middle|X=x\right] \nonumber \\
    &\quad - \frac{2\beta^2\mu^2}{\left(\beta-\sigma^2\right)\left(2\beta-\sigma^2\right)}
\end{align}
Now since 
\begin{align}
    \left(\mathbb{E}\left[X_\Delta\middle|X=x\right]\right)^2 &= \left(\exp\left(-\beta\Delta\right)\left(x-\mu\right) + \mu\right)^2 \\
    &= \exp\left(-2\beta\Delta\right)\left(x-\mu\right)^2 + \mu^2 + 2\mu\exp\left(-\beta\Delta\right)\left(x-\mu\right).
\end{align}
Combining these and using the definition of conditional variance, we get the desired result
\begin{align}
    \mathrm{Var}\left(X_\Delta \middle| X = x\right) &= \exp\left(-\left(2\beta - \sigma^2\right)\Delta\right)\left(x^2 - \frac{2\beta\mu}{\beta-\sigma^2}x + \frac{2\beta^2 \mu^2}{\left(\beta-\sigma^2\right)\left(2\beta - \sigma^2\right)}\right) \nonumber \\
    &+ \frac{2\beta\mu}{\beta-\sigma^2}\left(\exp\left(-\beta\Delta\right)\left(x-\mu\right) + \mu\right) - \frac{2\beta^2\mu^2}{\left(\beta-\sigma^2\right)\left(2\beta - \sigma^2\right)} \nonumber\\
    &- \exp\left(-2\beta\Delta\right)\left(\left(x-\mu\right)^2 - \mu^2 - 2\mu\exp\left(-\beta\Delta\right)\left(x-\mu\right)\right)
\end{align} 
Denote the one-step ahead conditional mean and -variance $F_i, \Phi_i$ respectively. Now, utilizing the method given in \cite[example 1.11]{StatisticalMethodsForSDE} we have approximate quadratic martingale estimation equations given by
\begin{align}
    G_N^{GBM} = \begin{bmatrix}
        \sum_{i = 1}^N \frac{\beta}{\sigma^2 X_{t_{i-1}}^2}\left(X_{t_i} - F_{t_i}\right)\\
        \sum_{i = 1}^N \frac{-1}{\sigma^2 X_{t_{i-1}}}\left(X_{t_i} - F_{t_i}\right)\\
        \sum_{i = 1}^N \frac{1}{\sigma^3 X_{t_{i-1}}^2}\left(\left(X_{t_i} - F_{t_i}\right)^2 - \Phi_{t_i}\right)
    \end{bmatrix}.
\end{align}
Solving the equation $G_N^{GBM} = 0$ then yields the approximate quadratic marginale estimator. This we do numerically using the \code{nleqslv}-method from the package of the same name \cite{nleqslv}.
\subsubsection{Dynamic part}\label{subsubsec:meanrevertingGBMDynamic}
In its dynamic part the stochastic differential equation for the process can be written as
\begin{align}
    \mathrm{d}X_t &= -\left(A\left(X_t - m\right) + \lambda_t\right) + \sigma X_t \mathrm{d}W_t, \\
    \lambda_t &= \lambda_0\left(1 - \frac{t - t_0}{\tau_c}\right)^\nu,
\end{align}
and as before we assume that $\lambda_0, \sigma, m$ can estimated or can be computed from other parameters in the same manner. The choice of splitting is the same as in \ref{subsubsec:squarerootDynamic}
\begin{align}
    \mathrm{d}X_t^{[1]} &= -\alpha(\lambda)\left(X_t^{[1]} - \mu(\lambda)\right)  \mathrm{d}t + \sigma X_t^{[1]} \mathrm{d}W_t, \label{eq:GBMSplit1} \\
    \mathrm{d}X_t^{[2]} &= - A \left(X_t^{[2]} - \mu(\lambda)\right)^2 \mathrm{d}t, \label{eq:GBMSplit2}
\end{align}
with $\alpha(\lambda) = 2\sqrt{\abs{A\lambda_t}}$ and $\mu(\lambda) = m + \sqrt{\abs{\frac{\lambda_t}{A}}}$.
Again, this splitting is obtained by letting the mean-reverting GBM be the linearization around the fixed point of the process. To get the Strang estimator we simply use kessler's method on the mean-reverting GBM in (\ref{eq:GBMSplit1}) and the same flow for (\ref{eq:GBMSplit2}) as in (\ref{eq:squareRootSplit2}) in composition. Then, the maximizer of the resulting pseudo-likelihood is once again the Strang-based estimator.
Alternatively, we can find the Strang estimator based on splitting the lamperti-transformed process. As in the stationary part let $Y_t:=\log(X_t)$. From Itô's formula we get
\begin{align}
    \mathrm{d}Y_t = - \left(\frac{A\left(\exp\left(Y_t\right)-m\right)^2 + \lambda_t}{\exp(Y_t)} + \frac{\sigma^2}{2}\right)\mathrm{d}t + \sigma \mathrm{d}W_t
\end{align}
By linearizing this process around the fix point of the drift we get the following splitting scheme
\begin{align}
    \mathrm{d}Y_t^{[1]} &= A_{\mathrm{linear\_part}}\left(Y_t^{[1]} - \log\left(\zeta\right)\right)\mathrm{d}t + \sigma \mathrm{d}W_t,\\
    \mathrm{d}Y_t^{[2]} &= \left(-\left(\frac{A\left(\exp\left(Y_t^{[2]}\right)-m\right)^2 + \lambda_t}{\exp\left(Y_t^{[2]}\right)} + \frac{1}{2}\sigma^2\right) \right. \nonumber \\
     &- \left. A_{\mathrm{linear\_part}}\left(Y_t^{[2]} - \log\left(\zeta\right)\right)\right)\mathrm{d}t,
\end{align}
where
\begin{align}
    \zeta &= \frac{2mA - \frac{1}{2}\sigma^2 + \sqrt{\sigma^4/4 - A\left(2m\sigma^2 + 4\lambda_t\right)}}{2A},\\
    A_{\mathrm{linear\_part}} &= - \frac{1}{\zeta}\left(A \zeta^2 - \lambda_t - Am^2\right).
\end{align}
We will solve the linear SDE with the well-known of it. The non-linear ODE's solution, the inverse of the solution and the derivative of the inverse of the solution will be found numerically with the fourth-order Runge-kutta. Putting together the pieces, we get the strang-estimator of the lamperti-transformed process in the usual manner.
\subsection{Skew t-distribution}
\subsubsection{Stationary part}
The process with the same noise as skew t-distribution pearson diffusion is assumed to be governed by this diffusion in the non-dynamic part
\begin{align}
    \mathrm{d}X_t = -\beta\left(X_t - \mu\right)\mathrm{d}t + \sigma \sqrt{\left(X_t^2 + 1\right)}\mathrm{d}W_t.
\end{align}
For estimation of the parameters in the diffusion, we use the Strang splitting. Define $Y_t := \sinh^{-1}(X_t)$, then by Itô's formula
\begin{align}
    \mathrm{d}Y_t = - \frac{1}{\cosh(Y_t)}\left(\sinh(Y_t)\left(\beta + \frac{1}{2}\sigma^2\right) - \mu\beta\right)\mathrm{d}t + \sigma \mathrm{d}W_t.
\end{align}
With the same splitting method as we have used the majority of the times, we get the splitting scheme
\begin{align}
    \mathrm{d}Y_t^{[1]} &= -\left(\beta + \frac{1}{2}\sigma^2\right)\left(Y_t^{[1]} - \sinh^{-1}\left(\frac{2\beta\mu}{2\beta + \sigma^2}\right)\right)\mathrm{d}t + \sigma \mathrm{d}W_t \\
    \mathrm{d}Y_t^{[2]} &= \left(\left(\beta + \frac{1}{2}\sigma^2\right) \left(Y_t^{[2]} - \tanh\left(Y_t^{[2]}\right) - \sinh^{-1}\left(\frac{2\mu\beta}{2\beta + \sigma^2}\right)\right) + \right. \nonumber \\
    &\left. \frac{\mu\beta}{\cosh\left(Y_t^{[2]}\right)}\right)\mathrm{d}t
\end{align}
\subsubsection{Dynamic part}
Turning to the dynamic part of the process, we want to estimate $A, \tau_c, \nu$ in
\begin{align}
    \mathrm{d}X_t &= -\left(A(X_t - m)^2 + \lambda_t\right)\mathrm{d}t + \sigma \sqrt{\left(X_t^2 + 1\right)}\mathrm{d}W_t,\\
    \lambda_t &= \lambda_0 \left(1 - \frac{t - t_0}{\tau_c}\right)^\nu.
\end{align}
Once again let $Y_t = \sinh^{-1}(X_t)$; we get from Itô's formula that
\begin{align}
    \mathrm{d}Y_t = -\frac{1}{\cosh(Y_t)}\left(A\left(\sinh(Y_t) - m\right)^2 + \lambda_t + \frac{\sigma^2}{2}\sinh(Y_t)\right)\mathrm{d}t + \sigma \mathrm{d}W_t.
\end{align}
We find the fix point of the drift and linearize the process around it, yielding the splitting
\begin{align}
    \mathrm{d}Y_t^{[1]} &= A_{\textrm{linear\_part}}\left(Y_t^{[1]} - b_{\textrm{intercept}}\right)\mathrm{d}t + \sigma \mathrm{d}W_t \label{eq:linearSplitTdist} \\
    \mathrm{d}Y_t^{[2]} &= \left(-\frac{1}{\cosh(Y_t^{[2]})}\left(A\left(\sinh(Y_t^{[2]}) - m\right)^2 + \lambda_t + \frac{\sigma^2}{2}\sinh(Y_t^{[2]})\right)\right.\\
     &\left.- A_{\textrm{linear\_part}}\left(Y_t^{[2]} - b_{\textrm{intercept}}\right)\right)\mathrm{d}t
\end{align}
where 
\begin{align}
    A_{\textrm{linear\_part}} &= \left(\sqrt{\frac{\sigma^4}{4} - 2A\left(2\lambda_t + m\sigma^2\right)}-\sigma^2\right)\\
    b_{\textrm{intercept}} &= \sinh^{-1}\left(\frac{\sqrt{\sigma^4 - 8A\left(2\lambda_t + m \sigma^2\right)} + 4 Am - \sigma^2}{4A}\right).
\end{align}
We solve (\ref{eq:linearSplitTdist}) using the solution to the OU-process, whereas the solution to the non-linear ODE along with its inverse and derivative of inverse is found numerically with the fourth order Runge-kutta.
\subsection{The scaled F-distribution}
\subsubsection{Stationary part}
In the stationary part of the process the pearson diffusion with the scaled F-distribution as its stationary distribution is governed by
\begin{align}
    \mathrm{d}X_t = -\beta\left(X_t - \mu\right)\mathrm{d}t + \sigma \sqrt{\left(X_t\left(X_t + 1\right)\right)}\mathrm{d}W_t.
\end{align}
The lamperti-transform is $Y_t := g(X_t, t) = 2 \sinh^{-1}\left(\sqrt{X_t}\right)$ and Itô's formula gives us
\begin{align}
    \mathrm{d}Y_t = - \frac{1}{\sinh(Y_t)}\left(\left(\beta + \frac{\sigma^2}{2}\right)\cosh(Y_t) - \beta\left(2\mu + 1\right)\right) \mathrm{d}t + \sigma \mathrm{d}W_t.
\end{align}
Continuing with the same heuristic of linearizing around the fix-points of the drift, we get the splitting
\begin{align}
    \mathrm{d}Y_t^{[1]} &= -\left(\beta + \frac{\sigma^2}{2}\right)\left(Y_t^{[1]} - \cosh^{-1}\left(\frac{2\beta\left(2\mu + 1\right)}{2\beta + \sigma^2}\right)\right)\mathrm{d}t + \sigma \mathrm{d}W_t, \\
    \mathrm{d}Y_t^{[2]} &= \left(\beta + \frac{\sigma^2}{2}\right) \left(Y_t^{[2]} - \cosh^{-1}\left(\frac{2\beta\left(2\mu + 1\right)}{2\beta + \sigma^2}\right) - \coth\left(Y_t^{[2]}\right) \right.\nonumber\\
    &+ \left. \frac{2\beta\left(2\mu + 1\right)}{\left(2\beta + \sigma^2\right)\sinh(Y_t^{[2]})}\right)\mathrm{d}t.
\end{align}
The flow is then found in the usual manner: The linear part is obtained as the solution to the OU sde, and the non-linear ODE along with its inverse etc. is solved with a fourth-order Runge-kutta. The maximizer of the resulting density of the flow is then the Strang-estimator.  
\subsubsection{Dynamic part}
Conversely, the dynamic part of the process, we need to estimate $A, \tau_c, \nu$ in
\begin{align}
    \mathrm{d}X_t &= -\left(A(X_t - m)^2 + \lambda_t\right)\mathrm{d}t + \sigma \sqrt{\left(X_t\left(X_t + 1\right)\right)}\mathrm{d}W_t,\\
    \lambda_t &= \lambda_0 \left(1 - \frac{t - t_0}{\tau_c}\right)^\nu.
\end{align}
With the transform $Y_t := 2\sinh^{-1}\left(\sqrt{X_t}\right)$ the resulting SDE is 
\begin{align}
    \mathrm{d}Y_t &= - \frac{1}{\sinh\left(Y_t\right)}\left(\frac{A}{2}\cosh^2(Y_t) + \left(\frac{\sigma^2}{2} - m - A\right)\cosh(Y_t) \right. \nonumber \\ 
    &+\left. 2\lambda_t + m + \left(2m^2 + \frac{1}{2}\right)A\right)\mathrm{d}t + \sigma \mathrm{d}W_t \label{eq:dynamicScaledFLamperti}
\end{align}
The flows are given analagously to earlier. The slope and intercept of the linear SDE is
\begin{align}
    A_{\mathrm{linear\_part}} &= -\zeta + m + A - \frac{\sigma^2}{2},\\
    b_{\mathrm{intercept}} &= \cosh^{-1}\left(\frac{\zeta}{A}\right),
\end{align}
with
\begin{align}
    \zeta = m + A - \frac{\sigma^2}{2} - \sqrt{\frac{\sigma^4}{4} + m^2 - \sigma^2m - A\left(\sigma^2 + 4 \lambda_t + 4 A m^2\right)}.
\end{align}
And the non-linear ODE is constructed as the residual of this linear SDE and (\ref{eq:dynamicScaledFLamperti}). All elements are solved in the usual manner and the strang-estimator defined completely analogously to earlier.
\subsection{The Jacobi diffusion}
\subsubsection{Stationary part}
During the non-dynamic part of the process, it follows the jacobi diffusion
\begin{align}
    \mathrm{d}X_t = -\beta\left(X_t - \mu\right)\mathrm{d}t + \sigma \sqrt{X_t\left(1 - X_t\right)}\mathrm{d}W_t.
\end{align}
Define $Y_t := 2 \sin^{-1}\left(\sqrt{X_t}\right)$ then 
\begin{align}
    \mathrm{d}Y_t = -\frac{1}{\sin\left(Y_t\right)}\left(\left(\frac{\sigma^2}{2}-\beta\right)\cos(Y_t) + \beta - 2\beta\mu\right)\mathrm{d}t + \sigma \mathrm{d}W_t,
\end{align}
by Itô's formula. The linearization around the fixpoint gives us the splitting
\begin{align}
    \mathrm{d}Y_t^{[1]} &= \left(\frac{\sigma^2}{2} - \beta\right)\left(Y_t^{[1]} - \cos^{-1}\left(\frac{2\beta\left(2\mu - 1\right)}{\sigma^2 - 2\beta}\right)\right)\mathrm{d}t + \sigma \mathrm{d}W_t,\\
    \mathrm{d}Y_t^{[2]} &= -\left(\frac{1}{\sin\left(Y_t^{[2]}\right)}\left(\left(\frac{\sigma^2}{2}-\beta\right)\cos(Y_t^{[2]}) + \beta - 2\beta\mu\right) \right. \nonumber \\
    &+ \left. \left(\frac{\sigma^2}{2} - \beta\right)\left(Y_t^{[2]} - \cos^{-1}\left(\frac{2\beta\left(2\mu - 1\right)}{\sigma^2 - 2\beta}\right)\right) \right)\mathrm{d}t .
\end{align}
and the strang-estimator is found in the usual manner.
\subsubsection{Dynamic part}
In the dynamic part we have the SDE
\begin{align}
    \mathrm{d}X_t &= -\left(A(X_t - m)^2 + \lambda_t\right)\mathrm{d}t + \sigma \sqrt{X_t\left(1 - X_t\right)}\mathrm{d}W_t,\\
    \lambda_t &= \lambda_0 \left(1 - \frac{t - t_0}{\tau_c}\right)^\nu.
\end{align}
Let $Y_t := 2 \sin^{-1}\left(\sqrt{X_t}\right)$ then by Itô's formula 
\begin{align}
    \mathrm{d}Y_t &= - \frac{1}{\sin(Y_t)}\left(\frac{A}{2}\cos^2(Y_t) + \left(\frac{\sigma^2}{2} + 2 Am - A\right)\cos(Y_t) \right. \nonumber \\
    &+ \left. A \left(\frac{1}{2} + 2m^2 - 2m\right) + 2\lambda_t\right) + \sigma \mathrm{d}W_t
\end{align}
Now, the splitting is done in the ususal manner. The linearization around the fix points yield the slope and intercept for the linear SDE
\begin{align}
    A_{\mathrm{linear\_part}} &= \zeta + \frac{\sigma^2}{2} - 2 Am - A,\\
    b_{\mathrm{intercept}} &= \cos^{-1}\left(\frac{\zeta}{A}\right),
\end{align}
where 
\begin{align}
    \zeta = A - \frac{\sigma^2}{2} - 2 A + \sqrt{\frac{\sigma^4}{4} - \sigma^2 A - 2 \sigma^2 A m - 2 A \lambda_t},
\end{align}
The ODE is found residually and solved using our fourth-order Runge-kutta.
\section{Benchmark}
In this section we show the results of a few benchmarks using the \code{microbenchmark} package \cite{microbenchmark}. The code was run on a machine with the following hardware and software.
\begin{table}[ht]
    \centering
    \begin{tabular}{@{}ll@{}}
    \toprule
    Specification      & Details                              \\ \midrule
    CPU Model          & Intel i7-4800MQ                 \\
    CPU Speed          & 800 MHz (min) / 3700 MHz (Max)     \\
    Number of Cores/Threads & 8 cores / 8 threads              \\
    RAM Capacity       & 16 GB                                \\
    RAM Type and Speed & DDR3, 1600 MT/s                      \\
    Storage Type       & SSD                                  \\
    Storage Capacity   & 512 GB                               \\
    GPU Model          & NVIDIA GeForce GT 730M              \\
    Operating System   & Linux Mint 21.3 x86\_64                   \\
    Kernel Version     & 5.15.0-100-generic                    \\
    R Version          & 4.3.3                                \\
    \bottomrule
    \end{tabular}
    \caption{Hardware specifications}
    \label{tab:specs}
    \end{table}