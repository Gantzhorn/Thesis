This discussion is structured into two semi-independent parts: We first summarize the results from the simulation study and reflect on how we can develop on the methods especially on the numerical optimization side. Furthermore, we mention two estimation methods that could be interesting to consider in future research.

Then we put the results from the estimation on the AMOC into perspective, contrasting it with previous findings. We add a robustness analysis to our findings in the same way  Here, we also discuss how one might go about selecting between models from \ref{table:ergodicDiffusions}. In particular, with our models as a starting point we discuss what type of role the stochastic part in these types of analysis can have. 

Additionally, we mention a couple of other ideas that can be used on these models in practice. In extension, we reflect on how one can think about these models in general.
\subsection{Simulation studies}
Our starting point for the discussion is a short look into the results shown in figures \ref{figure:parameter_precision_dynamic} and \ref{figure:overviewOfEstimatorsStationary}. As we noted it is not given that the behaviour shown in these graphs is representative on any scale. Additionally, the experiment only tweaked the number of samples through the temporal resolution. In a future study, it would be intersting to investigate how longer (or shorter) time series affects the estimators in an otherwise similar experiment. With our simulation schemes we can achieve this by changing the $t_0$- and $\tau_c$ parameters.

Another natural extension is to look into other evaluation metrics. We mostly considered the ARE and the computation time. One other metric we could have used more is the relative error. Another popular choice is just the difference, because it with proper normalization can be used to examine the distribution of the estimators more precisely. Although we have not stated them, allmost all of our estimators have asymptotic results, which in principle can be demonstrated in these kinds of studies.

Other than that, we could consider changing the BFGS-algorithm to other algorithms and what effect tweaking hyperparameters for these might have. In continuation, specifying the gradients for the various methods might reduce the number of steps required for convergence at the minimum. Additionally, it is likely that having a closed form of the gradient makes evaluating it less computationally heavy than our current finite-difference based algorithm. Albeit, in our current development the Ornstein Uhlenbeck is the only process for which we have implementations of both the likelihood and the score. However a quick test, shows that the optimization can experience an increase in performance, if we can provide the gradient to it. The result is shown in table \ref{table:smallOUExperiment}. To be able to apply the strategy outside of this case hinges on the Strang-based score being tractable. But if it were then we could use it in unison with the likelihood in the optimization.

 There is no doubt that at our current state of implementation it is not feasible to estimate with a varying $\nu$-parameter in the models. As seen in figure \ref{figure:error_count_nu_experiment} the optimization process is simply not robust enough to even try estimation on real data. It is probable that the challenges we have experienced stem from the fact that $\nu$ enteres into the likelhood in quite a complex manner. Confer figure \ref{figure:nu_plot}, the $\nu$-parameter shapes the overall evolution of the fixed point. What the plot also reveals is that relatively small changes around $\nu = 1$ results in quite different shapes, making estimation difficult.
 
 Without any prior knowledge the idea of starting optimization at $\nu = 1$ seems like the only fair way to do so. So before we find any reasons to deviate from this we stick with this stategy. However, this can obviously leave the optimizer with a difficult optimization problem; there is a chance we get stuck in local minima. To remedy this one option we could try is to penalize values of $\nu$ relatively far away from $1$. Due to the form (\ref{eq:lambdaRampDefinition}) straying further away from $\nu = 1$ results in progressively smaller changes in the shape for a change in $\nu$. In figure \ref{figure:nu_plot} this can be seen by the shape of the fixed points not being as different for $\nu = 2.27$ and $\nu = 4.48$ as for instance $\nu = 1$ to $\nu = 1.65$. Therefore penalizing values much larger than $1$ and relatively close to $0$ might be good choice to push the optimization away from these parts of the parameter space. 

When the optimization of $\nu$ is more robust another consideration to make is to question when there even is enough information in the time series to estimate $\nu$ reliably. Unfortunately due to the aforementioned issues, we can only illustrate this point intuitively here. Though to do so, we sample from the additive-noise model using the same parameters as in the illustration \ref{figure:nu_plot}. The $\sigma$-parameter is set to $0.3$ and we sample with temporal resolution $\Delta t = 1/24$
\begin{figure}[h!]
    \begin{center}
    \includegraphics[scale = .13]{figures/mu_simulations_discussion_plot.jpeg}
    \caption{Sample paths for varying $\nu$ with the same realizations from the brownian motion cut at different points in time.}
    \label{figure:mu_simulations_discussion_plot}
    \end{center}
\end{figure}\\
This example is of course constructed for sake of illustration and the following points might not always hold. However, figure \ref{figure:mu_simulations_discussion_plot} is a clear example of some of the difficulties that can arise with regards to estimation of the $\nu$-parameter. In each facet the vertical dashed line marks the point in time upto which the facet before it ran. The colored dotted lines in the plots are the unstable fixed points that if crossed before the tipping point, most likely results in the process experiencing noise-induced tipping. When this happens we can no longer predict a bifucartion tipping. Furthermore, due to noise-tipping the process is no longer close to the stable fixed point, whence there is no information about $\nu$ in the samples.

Conversely, we still need to have quite a long series of observations within the dynamic part of the process to be able to distinguish between the paths. We are only 240 observations within the dynamic part in the upper left facet in figure \ref{figure:mu_simulations_discussion_plot}, and we can barely tell the sample paths coming from very different values of $\nu$ from apart. That is at least visually. However, as our experiments have shown estimating values even moderately far away from $\nu = 1$ results in difficulties for the optimizer. 

All in all, we might need more sophisticated optimization methods than \code{optim} to have succes with estimating parameters such as $\nu$. This could be achieved how we have already discussed, while another option is using automatic differentiation lately made more easily applicable in \code{R} via the \code{torch} framework \cite{torch}, or even hand coding the optimizers.

Even another option is to develop the numerical optimizer even further, albeit its possibilites of finding fixed points for a range of values $t$ as required in the dynamical part are quite limited without some a priori given assumptions about the evolution of these fixed points. The strategy that might be applicable here is providing the specific form of $\lambda$ used throughout and use it to create sub-optimization problem. Though, we do not observe $\lambda$ directly, the sample paths should be close to the fixed point on average; assuming that the process does not tip due to noise, of course. With this we could perhaps use non-linear least squares to regress $\mu_t$ on the samples. We did experiment a bit with this with the method \code{stats::nls} but only got limited success. Solving the non-linear least squares at every iteration of the optimization is rather computationally expensive and it was quite error-prone: At some iterations the \code{stats::nls} failed due to other parameters such as $A$ making the problem infeasible. To avoid this we could for instance adopt the penalization of $A$ used in \cite{Ditlevsen2023}.

Another thing we tried was making the numerical Strang method purely numerical. That is, it should be implemented such that the user merely provides the data, the specific model one wants to fit and the usual values required for optimization. However, the user does not provide the lamperti SDE nor even the lamperti-transform (\ref{eq:lampertiDefinition}) here. To achieve this the method must construct these numerically, which we actually managed to do. Yet, the computational speed of the method became the bottleneck of development; it took too long to evaluate the likelihood even once for the method to ever be viable. Now, the raison d'être of the numerical methods are that they allow us to use the methods without going through extensive calculations. And while simplifying the calculations necessary down to nothing whatsoever would be nice, the method as is still provides a significant reduction in number of manual computations needed. Furthermore, it is uncertain if we would be able to match the results shown in figure \ref{figure:ARE_dist_linear_noise} or \ref{figure:ARE_dist_numeric_F_diffusion}, if we automated the calculations further. In addition, it is to be expected that a user wanting to apply these methods would be able to find the lamperti-transform and compute the lamperti SDE with Ito's formula, which is all the numerical Strang based method requires. This further cements keeping the numerical Strang method as it is.

We already touched on the idea of using the Strang splitting scheme to construct a score function. Now, a third way to use this scheme could be to construct martingale estimation equations for stochastic differential equations that are more complicated than the Pearson diffusions, e.g. the dynamic part of the process. As with our idea of using Kessler's method on the stochastic differential equation in the splitting, this method does not use the lamperti-transform of the process. To see this more easily, consider the square-root based dynamical part of the saddle-node normal form model
\begin{align}
    \mathrm{d}X_t = -(A(X_t - m)^2 + \lambda_t)\mathrm{d}t + \sigma\sqrt{X_t}\mathrm{d}W_t \label{eq:squareSplittingDiscussion}
\end{align}  
The following splitting is also in section \ref{subsubsec:squarerootDynamic}
\begin{align}
    \mathrm{d}X_t^{[1]} &= -\alpha(\lambda)\left(X_t^{[1]} - \mu(\lambda)\right)  \mathrm{d}t + \sigma \sqrt{X_t^{[1]}} \mathrm{d}W_t, \label{eq:squareRootSplit1_discussion} \\
    \mathrm{d}X_t^{[2]} &= - A \left(X_t^{[2]} - \mu(\lambda)\right)^2 \mathrm{d}t, \label{eq:squareRootSplit2_discussion}
\end{align}
with $\alpha(\lambda) = 2\sqrt{-A\lambda_t}$ and $\mu(\lambda) = m + \sqrt{-\frac{\lambda_t}{A}}$. As we mention in that part of the appendix this is applying the splitting heuristic from \cite{SplittingSchemes} to (\ref{eq:squareSplittingDiscussion}). However, what this gives us is an SDE (\ref{eq:squareRootSplit1_discussion}), for which we can construct martingale estimation equations. For the Strang composition, we recall that based on figure \ref{figure:StrangAndLieTrotterPlot} the flow is a non-linear transform of the solution to the SDE (\ref{eq:squareRootSplit1_discussion}). So constructing the estimation equations for the whole flow is a matter of using a result about the relationsship between eigenfunctions, $p_n(x)$ and eigenvalues $\lambda_n$ for a process such as $X_t^{[1]}$ and transformations of that process with functions such as $\varphi_2$. It turns out \cite[remark on p. 41]{StatisticalMethodsForSDE} that in this case the eigenfunctions are $p_n\left(\varphi_2^{-1}(x)\right)$ and the eigenvalues the same. Still, it is unlikely that we would be able to get the conditonal mean- and variance  of the flow from these eigenfunctions and eigenvalues, so in the remaining calculations one has to use a few more results from the theory of estimation equations than what have been presented here.
\subsection{Tipping of the AMOC}
Taking figure \ref{figure:surival_curve_taus} and table \ref{table:tipping_quantiles} as our starting points of the discussion, it is clear that all the models indicate tipping to be \textit{more} than likely within the next 140 years. By "likely" we understand the 66\% confidence interval that is used by the Intergovernmental Panel on Climate Change \cite{Ditlevsen2023} and accross all models and fingerprints there is at least an $83.5\%$ chance of tipping occuring before the end of year $2169$. What specific range of years the tipping of the AMOC is likely within various a bit according to the different models. Still, all the \textit{estimates} from the data lie within medio 21st century and primo 22nd century signifying some form of overall agreement.

We did not investigate penalization on the models, but it would be interesting to adopt the penalization on $A$ into the $t$-diffusion based model as well. To get an idea of how this could affect the estimates of the tipping time consider pairs of estimates of tipping year and $A$ stratified after model
\begin{figure}[h!]
    \begin{center}
    \includegraphics[scale = .082]{figures/correlation_between_A_and_tau_plot.jpeg}
    \caption{Pairs of estimates of tipping year and $A$ grouped by model and fingerprint}
    \label{figure:correlation_A_and_tau}
\end{center}
\end{figure}\\
The straight lines in the graphs are linear normal models fitted to the stratified samples they share color and facet with. In all cases there is a clear negative correlation between the estimate of $A$ and -the tipping year; small values of $A$ tend to be have tipping estimates far in the future and vice versa. Indicating that if we want to avoid heavy-tailed estimators we might want to penalize values of $A$ below 1 as done in the paper.

However, as seen on the graph in figure \ref{figure:correlation_A_and_tau} this creates a clear bias towards $A = 1$. Actually, one could argue that the penalizations might have hurt the optimization somewhat. Note the clear cluster of points forming around $A = 1$ in the middle graph of figure \ref{figure:correlation_A_and_tau}. This is likely a result of penalization of values below $A$ resulting in the optimization returning values \textit{really} close to $A = 1$. In fact, a closer investigation reveals that penalization results in $12.4\%$ of estimates of $A$ being closer than $0.0005$ units away from $1$, whereas the proportion of estimates of $A$ with an absolute distance of just $0.000025$ to $1$ is still $7.9\%$. While we of course expect the distribution to shift towards $1$ for values that would have been much smaller than $A = 1$, so many values this close to $1$ is a bit concerning and could indicate that there is something wrong with the optimization or penalization. 

Yet, looking at the source code nothing particular from the objective function has stood out in this regard. In the optimization itself one thing to note is that the initial value of $A$ is 1, as in our application. There is a chance that the many estimates around the value 1 is caused by the optimization not being explorative enough. As mentioned, the paper uses the \code{Nelder-mead}-algorithm for optimization. This is an algorithm that for poor initial values can get stuck in the optimization. While this exploitive nature ensures we do not get the extreme estimates that our methods sometimes suffer from, there is the risk of getting stuck in local minima. Though, implementing a tracer into the objective function allows us to retrieve the parameters and objectives at every iteration and see whether this really is the case. We sketch the trace of each of the parameters, where we have used the penalized additive model on the AMOC2-fingerprint with starting values $A = 1$ and $\tau_c = 100$. 
\begin{figure}[h!]
\begin{center}
    \includegraphics[scale = .075]{figures/trace_plot.jpeg}
    \caption{Trace plot of parameter values at each step of the optimization showing the relative deviation from the initial values $A = 1$ and $\tau_c = 100$.}
    \label{figure:trace_plot}
\end{center}
\end{figure}\\
In this example there is no lack of exploration of the $A$-parameter; the optimizer also searches among parameter values far away from the starting value. The same thing can be said about $\tau_c$ though its trace appear more guided overall. Of course, this is only one trace and we would need to examine more traces to reject this potential problem more confidently

Regardless of whether it is caued by a undesired having many values of $A$ around $1$ limits the range of values of the tipping, shifting $\tau$ in the specific direction as argued by the negative correlation in figure \ref{figure:correlation_A_and_tau}. Again, there is nothing inherently wrong with the correlation of the estimates, but it becomes an issue only in unison with something problematic in the optimization phase such as the potential problem illustrated here. And althouhg we have not been able to establish the underlying cause, there are still reason to belive that with this proportion of $A$ so close to 1 something unintended is going on during optimization.  

Now apart from this, we also discovered a minor bug in the implementation of the objective function for the dynamic part of the additive model that is in the supplementary material of the paper. The bug has since been corrected and although the estimates are a bit different, it has been checked that the distribution of the tipping time stays roughly the same after the correction. This means the arguments and comparisons made with results from the paper still hold. For this reason, we do not redo all the computations to recreate the distributions of \cite{Ditlevsen2023}. To this end it is worth mentioning that there are other minor differences in the choices made between the estimation methods of the paper and this project such as whether $\sigma$ or $\sigma^2$ is estimated, what optimization algortihm is used etc. These differences can compound making the results quite different overall. 

With this in mind, we get $2054$ for the tipping time with the corrected likelihood method using the exact penalization from the paper. This is rather close to the original result, confer table \ref{table:tipping_quantiles}, which further justifies the use of the results from the paper in spite of these finds. Again the result presented comes with the caveat that the implementation from the paper might not agree completely and it might have been a bit naïve of us to select the exact same penalization value for the optimization and not determine it via cross-validation for instance. However, its done in the way to try and ensure some overall comparability to the original implementation.

Returning to the selecting between the two models; in our initial discussion (below figure \ref{figure:OU_t_diffusion_QQ_plot}) we favored the $t$-diffusion model as its uniform residuals resembled that of the standard gaussian better overall - suggesting a better fit. Confer table \ref{table:tipping_quantiles} the models agree on the estimated tippings times and their confidence intervals - for the most part. On the AMOC2 data they are almost in complete agreement. Interestingly, the additive model generally puts tipping later on the AMOC1 fingerprint and earlier on the AMOC3 in comparison with the $t$-diffusion based model. A strengh in the $t$-diffusion model here is that it is more robust against choice of fingerprint than the additive model; this can also be seen by how close the survival curves in figure \ref{figure:surival_curve_taus} is.

Generally, we have argued for the $t$-diffusion model. It seems to provide the better fit overall, while also ensuring robustness without the need to penalize specific parameters. However, with the limited tools we have to differentiate between the models, the real significance of using multiple different stochastic terms with the same deterministic drift, as we have with the construction \ref{eq:standardStochasticForm}, is that it adds a different layer to the robustness analysis. If one can illustrate an effect in the deterministic part of a stochastic differential equation using several models with different diffusion terms, then it would strenghen our belief in that estimate as it is clearly less dependent on the model one uses. We succeeded in doing so with two models, though, preferably we would like to be able to apply other models as well, but of the models we have shown here only two is applicable on negative observations. There might be ways to adapt the diffusion-terms in the other models to extend them to reals, while still keeping their structure. Another solution could be to use some monotone map from the reals onto the positive real numbers, e.g. the exponential function. As our domain knowledge on climate physics is somewhat limited, we refrain from doing so here.

Instead of letting the ramping of the bifurcation parameter, $\lambda_t$ depend on time in a way that bifurcation inevitably occurs it could be model using say a linear normal model. This allows us to use covariates e.g. the $\mathrm{CO}_2$ levels in the atmosphere. A specific models that we have thought about is the following
\begin{align}
    \lambda_x= \beta_2 x + \beta_1, \label{eq:alternativeLambda}
\end{align}
where $x$ is the global mean $\mathrm{log}$-$\mathrm{CO}_2$ in the atmosphere. The objective here would be to estimate $\beta_1$ and $\beta_2$ and we would find the critical value by solving $\lambda_x = 0$ for $x$, i.e. $\hat{\lambda_c} = -\frac{\hat{\beta_1}}{\hat{\beta_2}}$; giving us the $\mathrm{CO}_2$ level at which tipping occurs instead of the point in time. A minor point to note here is that $\hat{\lambda_c}$ is not a very well behaved estimator. Amongst other things, being the ratio of two gaussian variables it has no mean.

Nevertheless, this model would not only be different to estimate in, it would somewhat shift our perspective in the tipping model. In the time to tipping type models there are of course the built-in assumption in the ramping (\ref{eq:lambdaRampDefinition}) that the development in the past continues going forward and the tipping time should of course be taken with this caveat. Though, what exactly drives the evolution of $\lambda_t$ is difficult to reason about, so it is hard to intuitively see, if the evolution in the future follows the expected path or deviates. 
On the other hand, a tipping $\mathrm{CO}_2$-value could be easier to communicate and we can easily measure if the concentration of $\mathrm{CO}_2$ in the atmosphere is below the threshold. 

Furthermore, the two models invites us to think in completely different ways in the first place: That ramping does not necessary continue can be hard to remember for a layperson; in that light a tipping time type model seems like a crystal ball predicting an inevitable event in the future. On the other hand, with the tipping level model it is easy fall into the trap of seeing tipping as a phenomenon that \textit{necessarily} occurs at specific $\mathrm{CO}_2$ levels. That is assuming a form of causality, the model does not let us reason about. However, where the tipping level type model excels it that it encourages us think about the system in structured manner. One could for instance ask questions about what levels of $\mathrm{CO}_2$ we should avoid for it to be "likely" that the system does not reach a tipping point. 

Finally, the two models could, of course, be used in conjunction; checking if the time of tipping alings well with our current $\mathrm{CO}_2$-projections. That is does the projected time for our reaching the critical $\mathrm{CO}_2$-level agree with the critical time in the tipping time type models.

\subsection{Conclusion}