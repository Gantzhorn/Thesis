\subsection{Saddle-node bifurcation and Tipping Point Estimation}
In the study of dynamical systems, we refer to how changes in parameters affect the qualitative structure of the flow as bifurcations and the parameter values that results in the changes as bifurcations points. In applications, we sometimes denote these tipping and tipping points, respectively, to get more metaphorical- and intuitive terms. \cite{Strogatz2019_gv} Nevertheless, we only consider the so-called Saddle-node bifurcations, defined qua the normal form
\begin{align}
    \mathrm{d}X_t &= \lambda \pm X_t^2.
\end{align}
The fixed points of the system are $x^* = \pm \sqrt{\abs{\lambda}}$ regardless sign in the system. There is always one stable- and unstable fixed point. The table fix point is the negative branch of the root for the positive version of the normal form, whereas the opposite is true for the negative version.
For values of $\lambda$ sufficiently close to the bifurcation point, $\lambda_c = 0$, any system that has the saddle-node bifurcation characteristic is approximated well by 
\begin{align}
    \mathrm{d}X_t &= -\left(\lambda \pm A\left(X_t - m\right)^2\right), 
\end{align}
with $m = \mu \pm \sqrt{\left|\frac{\lambda}{A}\right|}$ and $\mu$ is the stable fix point of the system. \cite{Ditlevsen2023}. In our applications, the sign of the parameters, $A, \lambda$, will always be different, thus we can simplify the notation in the model to
\begin{align}
    \mathrm{d}X_t &= -\left(A\left(X_t - m\right)^2 + \lambda\right), 
\end{align}
where $m = \mu \pm \sqrt{-\frac{\lambda}{A}}$; and the sign is the same as the sign of $A$. Again, $\mu$ is the stable fix point of the system. Now, to incorporate uncertainties into the model, we add a noise-term driven by a brownian motion
\begin{align}
    \mathrm{d}X_t &= -\left(A\left(X_t - m\right)^2 + \lambda\right) + \sigma^2(X_t, t)\mathrm{d}W_t, 
\end{align}
where $\sigma^2(X_t, t)$ is a function that dictates how the noise enters the system. This is a stochastic differential equation; for a motivation of this construction see \cite[Chapter 3.1-3.2]{Srkk2019}.
\subsection{Pearson diffusions}
A pearson diffusion is a solution to a stochastic differential equation on the form
\begin{align}
    \mathrm{d}X_t = -\beta \left(X_t - \mu\right)\mathrm{d}t + \sigma\sqrt{\left(aX_t^2 + bX_t + c\right)}\mathrm{d}W_t, \: \beta, \sigma > 0.
\end{align}
For appropriate choices of $a, b, c$ making the square-root well-defined in the state space of $X_t$. These diffusions are a well-studied class of stochastic differential equations. In this thesis we focus on the ergodic pearson diffusions. One can show that this is class of six special diffusions \cite[p.36]{StatisticalMethodsForSDE}. Amongst other things, we consider the Lamperti-transform of these processes defined as
\begin{align}
    \psi\left(X_t, t\right) := \int_{\xi}^{X_t} \frac{\mathrm{d}x}{\sqrt{\left(ax^2 + bx + c\right)}}. \label{eq:lampertiDefinition}
\end{align}
For some appropriate $\xi$ in the state space of the respective diffusions. Now by Itô's formula
\begin{align}
    \mathrm{d}\psi\left(X_t, t\right) = - \frac{1}{\sqrt{\left(aX_t^2 + bX_t + c\right)}}\left(\beta\left(X_t - \mu\right) + \frac{\sigma^2}{4}\left(2aX_t + b\right)\right)\mathrm{d}t + \sigma \mathrm{d}W_t.
\end{align}
We define $Y_t := \psi\left(X_t, t\right)$. However, in order to commence, we need to invert $\psi\left(X_t, t\right)$ and this obviously has to be handled casewise. We sketch a quick overview of the processes
\begin{table}[h!]
    \begin{center}
    \begin{tabular}{lllll}\hline
    \textbf{Name} & \textbf{Diffusion term} & \textbf{Lamperti-transform} & \textbf{State space}\\ \hline
    Ornstein-Uhlenbeck  & $\sigma$  & $X_t$ & $\mathbb{R}$ \\
    Square-root process & $\sigma\sqrt{X_t}$  & $ 2\sqrt{X_t}$ & $\mathbb{R}_{>0}$ \\
    Mean-reverting GBM  & $\sigma X_t $  & $ \log\left(X_t\right)$  & $\mathbb{R}_{>0}$ \\
    Skew t-diffusion  & $\sigma\sqrt{X_t^2 + 1}$  & $ \sinh^{-1}(X_t)$ & $\mathbb{R}$\\
    Scaled F-diffusion  & $\sigma\sqrt{X_t\left(X_t + 1\right)}$  & $ 2\sinh^{-1}\left(\sqrt{X_t}\right)$ & $\mathbb{R}_{>0}$ \\
    Jacobi-diffusion  & $\sigma\sqrt{X_t\left(1 - X_t\right)}$  & $ 2\sin^{-1}\left(X_t\right)$ & $(0, 1)$ \\ \hline
    \end{tabular}
    \caption{Overview of the ergodic pearson diffusions}
\end{center}
\end{table}\\
To see the expressions for $\mathrm{d}Y_t$ for each of the diffusions refer to appendix \ref{sec:AppendixEstim}. The lamperti-transformed process is instrumental to our later estimation as it per construction has additive noise, which we can exploit. Confer these concrete diffusion terms it is clear that \ref{eq:lampertiDefinition} is always well-defined; that is, the process is reducible.  We note that some of the diffusions are not ergodic for any choice of parameters. Whether there are any conditions for ergodicity and what these are depends on the diffusion in question; the condition is on the parameters. For instance, the Ornstein-Uhlenbeck is always ergodic and has invariant distribution $\mathcal{N}\left(\mu, \frac{\sigma^2}{2\beta}\right)$, whereas the square-root process is ergodic exactly when $2\beta\mu\geq \sigma^2$ and the invariant distribution here is $\Gamma\left(\frac{2\beta\mu}{\sigma^2}, \frac{2\beta}{\sigma^2}\right)$. \\

Later, we also need to be able to calculate moments up to the second order. In order to do so, we introduce the so-called Infinitesemal generator of a stochastic process
\begin{align}
    \mathcal{L}\varphi(x) = b(x, t) \varphi' + \frac{1}{2}\sigma^2(X_t, t)\varphi'',
\end{align}
where the derivatives are taken with respect to $x$ and $\varphi$ is a suitably regular function. We say that $\varphi$, $\lambda$ are eigen functions and -values for the process respectively if
\begin{align}
    \mathcal{L}\varphi = - \lambda \varphi,
\end{align}
Under mild regulatory conditions \cite[theorem 1.16]{StatisticalMethodsForSDE} then gives a method to derive the moments as
\begin{align}
    \mathbb{E}\left[\varphi(X_{t_k + 1}) \middle | X_{t_k}\right] = \exp\left(-\lambda t\right)\varphi \label{eq:momentConditions}
\end{align}
For typographical reasons, the dependence on the parameters have been suppresed in both $\varphi$ and $\lambda$. Additionally, Forman and Sorensen \cite{FormanSorensen2008} showed that for our ergodic diffusions, the eigenfunctions are all polynomials. Because of this, it is possible to derive any conditional moment of these processes, in spite of the fact that  the transition densities themselves are unknown. It is later going to be evident that we only need the first two eigenfunctions and -values - that is the ones from the first- and second order polynomials.
\subsection{Inference for stochastic differential equations}
Regardless of the method in question, inference about parameters in stochastic differential equations are quite often done by leveraging the markov property of Itô processes. This means that in order to estimate the parameter we need the transition density or approximations thereof.
To this end, inference is traditionally done qua the Euler-maruyama scheme. In this thesis, we only used the estimator based on this scheme in the initial development; it is fairly easy to derive, implement and it is computationally quite efficient. Yet, the estimator is biased even for moderately large stepsizes, and quite notably so in non-linear models \cite{SplittingSchemes}. Instead, we consider two other means of estimation 
\subsubsection{The Strang likelihood}
The Strang based estimator is a method based on splitting schemes. Other similar methods exists. However, for one-step predictions of transition densities Strang is proven to be superior; compare \cite[Proposition 3.4 and 3.6]{SplittingSchemes}. For one dimensional diffusions with additive noise, splitting schemes work by splitting the process into a linear SDE and a non-linear ODE
\begin{align}
    \mathrm{d}X_t^{(1)} &= -\beta(\theta)\left(X_t^{(1)} - \mu(\theta)\right)\mathrm{d}t + \sigma \mathrm{d}W_t, &&X_t^{(1)} = x_0, \\
    \mathrm{d}X_t^{(2)} &= N\left(X_t^{(2)}\right)\mathrm{d}t, &&X_t^{(2)} = x_0, \label{ODE_Split}
\end{align}
where $N$ is some non-linear function that also might depend on the parameters. Although the splitting is not unique and is irrelevant asymtotically, the choice might have an impact with finite samples. Additionally, one can imagine that some splittings are numerically more well-behaved. For the most part, we use the heuristic provided in \cite[section 2.3 and 2.5]{SplittingSchemes}. That is, we find the linear SDE be the linearization around the fix points of the drift; the ODE is then found as the residual of the original SDE and our linear SDE. In the one dimensional case, the solution with stepsize, $\Delta t$, to the linear SDE is given by the flow
\begin{align}
    \varphi_{\Delta t}^{(1)}(x) = \exp\left(-\beta\left(\theta\right) \Delta t\right)\left(x - \mu\left(\theta\right)\right) + \mu\left(\theta\right) + \xi_{\Delta t},
\end{align}
with $\xi_{\Delta t}\sim\mathcal{N}\left(0, \Omega_{\Delta t}\right)$. That is, the flow is gaussian with mean and variance
\begin{align}
    \mu_{\Delta t}(x; \theta) &= \exp\left(-\beta\left(\theta\right) \Delta t\right)\left(x - \mu\left(\theta\right)\right) + \mu\left(\theta\right) \label{linearSDEMean}\\
    \Omega_{\Delta t} &= \frac{\sigma^2}{2\beta}\left(1 - \exp\left(-2\beta\left(\theta\right)\Delta t\right)\right), \label{linearSDEVariance}
\end{align}
where the latter is calculated using \cite[equation (6)]{SplittingSchemes}. For our purposes the solution to (\ref{ODE_Split}) exists and is unique; with stepsize, $\Delta t$, we denote it $\varphi_{\Delta t}^{(2)}$. The conditions for this is provided in \cite[Assumption (A1) and - (A2)]{SplittingSchemes}. Then the Strang splitting scheme gives the approximation of the transition
\begin{align}
    X_{t_{i+1}}^{(S)} = \varphi_{\Delta t / 2}^{(2)}\left(\mu_{\Delta t}\left(\varphi_{\Delta t/2}^{(2)}\left(X_{t_{i}}^{(S)}\right); \theta\right) + \xi_{\Delta t}\right).
\end{align}
This is a non-linear transformation of a gaussian variable; so by the density transformation theorem, the flow gives us the following negative pseudo-loglikelihood 
\begin{align}
    l^{[S]} &= -\log\left(g\left(\left(\varphi_{\Delta t / 2}^{(2)}\right)^{-1}\left(X_{t_{i+1}}\right); \mu_{\Delta t}\left(\varphi_{\Delta t/2}^{(2)}\left(X_{t_{i}}\right); \theta \right), \Omega_{\Delta t} \right) \right) \nonumber \\
    &- \log\left(\partial_x \left(\varphi_{\Delta t / 2}^{(2)}\right)^{-1} \right), \label{Strang_likelihood}
\end{align}
and we say that the $\theta$ that minimizes this expression is the Strang-based estimator. In (\ref{Strang_likelihood}) $g$ is the density of the gaussian distribution with the specified mean and variance.
\subsubsection{Approximately Optimal Martingale Estimation Equations}
Other than using the strang splitting, we also estimate the parameters by means of the following approximately optimal martingale estimation functions \cite[Example 1.11]{StatisticalMethodsForSDE}.
\begin{align}
    G_N^{\circ} &= \sum_{i = 1}^N 
    \left(
        \frac{\partial_\theta\left(X_{t_{i-1}};\theta\right)}{\sigma^2\left(X_{t_{i-1};\theta}\right)}
    \right) \left(X_{t_{i}} - \mathbb{E}\left[X_{t_{i}} \middle| X_{t_{i-1}} = x\right]\right) \nonumber \\
    &+ \frac{\partial_\theta\sigma^2\left(X_{t_{i-1}}; \theta\right)}{2\sigma^4\left(X_{t_{i - 1}}\right)\Delta t}\left(\left(X_{t_{i}} - \mathbb{E}\left[X_{t_{i}} \middle| X_{t_{i-1}} = x\right]\right)^2 - \textrm{Var}\left[X_{t_{i}} \middle| X_{t_{i-1}} = x\right]\right) \label{eq:approximatelyOptimalMartingale}
\end{align}
\subsection{Numerical Optimization in \code{R}}
For each part of the process, we optimize a bit differently. However, in both cases the optimization is done with \code{stats::optim} \cite{Rlang} and our optimization methods are built as a wrapper around this function. To match the optimization done in \cite{Ditlevsen2023}, we use the Nelder-Mead algorithm in the dynamic part. In the stationary part, on the other hand, we use the BFGS-algorithm for its robustness. The wrapper is implemented such that it is possible to supply any of the likelihood functions to it. In addition, one can specify any other optimization algorithm that is implemented in \code{optim}. For the dynamic part, we also have to provide the values for $\alpha_0, \mu_0, \sigma$. We do this in the form of the estimated values for these from the stationary part of the processes. This part also dynamically chooses to estimate the $\nu$-parameter dependending on the dimension of the initial values given to the optimizer. If the dimension is two then we assume $\nu = 1$, while a dimension of three allows the optimizer estimate it. With regards to the numerical stability of the implementations we do a few things. Firstly, terms including $\exp\left(x\right) - 1$ or its additive inverse show up in many of our formulas. This is for instance the case in (\ref{linearSDEVariance}). As the arguments for the exponential function often is quite close to zero in these applications, we need to take care in order to avoid catastrophic cancellation. For this purpose, we make use of the \code{base::expm1} method in \code{R}, which calls the C-function of the same name \cite{cppreference_expm1}. Had we not done this, we would risk our program crashing, because (\ref{linearSDEVariance}) would due to catastrophic cancellation evaluate to zero.
\subsection{Model diagnostics}
When we are in a simulation setting, we assess the precision of our estimation methods using the mean of the absolute relative error over a number of simulations, $M$, each with sample size, $N$. This is for the $i$th-coordinate in our parameter vector defined as
\begin{align}
    \mathrm{ARE}\left(\theta_N^{(i)}\right) = \frac{1}{M}\sum_{j = 1}^M\frac{\left|\theta_{N,j}^{(i)} - \theta_{0,j}^{(i)}\right|}{\theta_{0,j}^{(i)}}.
\end{align}
Of course, we are not able to calculate this quantity when we do not have access to the ground-truth parameters. Instead, we use uniform residuals. These require that we have a transition density, $p_\theta(x|\Delta t, x_0)$. However, in most cases we must approximate this. Nevertheless, we then also have an approximation for the conditional distribution function, $F_\theta(x|\Delta t, x_0)$, and we know $F_\theta(X_{t_{i}}|\Delta t, X_{t_{i - 1}})\sim \mathrm{Unif}(0,1)$ if $X_{t_{i}}|X_{t_{i - 1}} \sim p_\theta$. Transforming this quantity with the quantile function of the standard gaussian distribution; we may assess the fit using ordinary Q-Q plots.\\
As the approximately optimal martingale estimation functions approximate the score function, we do not have any approximation of the transition density here. Even still, we can do diagnostics, but only of the square-root process. To see how, let $X_t$ to be governed by the square-root process, then
\begin{align}
    Y_{t_{i + 1}} := \frac{4\beta}{\sigma^2\left(\exp\left(-\beta \Delta t\right) - 1\right)}X_{t_{i + 1}}
\end{align}
has transition density of a non-central $\chi^2$-distribution with $\frac{4\beta\mu}{\sigma^2}$ degrees of freedom with non-centrality parameter $Y_{t_k}\exp\left(-\beta \Delta t\right)$ \cite[Equation (5.68)]{Srkk2019}. For the other diffusions, we do not have an analagous property to exploit, and we may only assess the score function based estimators by ensuring that the estimates are consitent with methods based on the transition density; for which we use the uniform residuals.